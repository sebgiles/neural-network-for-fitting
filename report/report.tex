% Compilare con PDFLaTeX
\documentclass[10pt,final]{siamltex}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{amsmath,amssymb}
%\usepackage{gensymb}
\textwidth = 16cm
\usepackage{pstricks}
%\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[breakable]{tcolorbox}
\usepackage{doi}
\usepackage{caption}%http://ctan.org/pkg/caption
\captionsetup[table]{format=plain,labelformat=simple,labelsep=period}%

%
\begin{document}
\title{Glomerular filtration rate estimation by a novel binning-less bivariate isotonic statistical regression method}
\author{Sebastian~Giles, Simone~Fiori%
\thanks{S. Giles is with the School of Information and Automation Engineering,
Universit\`{a} Politecnica delle Marche,
Via Brecce Bianche, I-60131 Ancona (Italy).
\newline\indent
S. Fiori is with Dipartimento di Ingegneria dell'Informazione,
Universit\`{a} Politecnica delle Marche,
Via Brecce Bianche, I-60131 Ancona (Italy).
\newline\indent
This draft is dated \today.}}
\maketitle
\def\bbbr{\mathbb{R}}
\def\bbbx{\mathbb{X}}
\def\bbby{\mathbb{Y}}
\def\mdef{{\stackrel{{\mathrm{def}}}{=}}}
% Footnotes in non-numerical fashion
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
\def\to{\mathbf{\ to\ }}
\setcounter{footnote}{1}
%
%
\begin{abstract}
  Bivariate statistical regression is a method for finding the relationship between unpaired sets of data based on statistic distribution matching. In the present report an  algorithm is proposed to eliminate binning from the previously published procedure. The method is then applied to correlate glomerular filtration rate (GFR) to serum creatinine concentration. GFR is an important indicator of kidney function. As direct measurement is highly impractical there is considerable interest in developing formulas to estimate it from parameters which are easier to obtain.
\end{abstract}
%
\section{Introduction}\label{intro}
Several real-world phenomena lack accurate mathematical descriptions \cite{strontium, seismic}. In these cases, it is not possible to predict the value of a variable by plugging known quantities in to an analytically derived expression, therefore empirical methods must be employed to develop a model of the observed process. The most common set of tools used to infer a functional relationship between variables is regression analysis \cite{control, ts}. Here only bivariate regression techniques will be considered. The amount of available data is assumed to be enough to explain the relevant statistical features of the phenomenon underlying the data.

The problem that triggered the present research is the estimation of glomerular filtration rate (GFR), which is used as an indicator of kidney function and is relevant for assessing progression of renal disease. It is also frequently required for evaluating optimal dosage for medications \cite{gfralb, gfrmed}. However, determination of true GFR is time-consuming, costly, and difficult to perform \cite{mgfr, mgfr2}. Thus, there is considerable interest in developing models to estimate GFR using simpler parameters such as age, weight, height, gender and values which can be more conveniently measured as part of a typical blood test.

Traditional forms of regression are based on some parametrised function whose graph is made to lie reasonably close to the points making up the experimental dataset. Values for the parameters are typically found using the least squares method \cite{book}.

Isotonic regression allows greater freedom for the regression curve to fit data by constructing a piecewise linear function, described by a lookup table (LUT). Overfitting is avoided by requiring the function to be monotonic, which is reasonable whenever the underlying physical phenomenon is inherently monotonic (such as the relationship between percentage body fat and waist circumference \cite{fat}). Various algorithms can be used to find the LUT values that satisfy a least squares condition \cite{bestchak, pava}.

Statistical bivariate regression (SBR) constitutes an improvement over isotonic regression. Its advantages derive from the fact that it relies on finding the relationship between the statistical distributions of the variables. SBR is not based on a least squares method so it does not require data to be associated in ordered pairs. This makes it ideal for correlating two quantities that cannot be both measured on the same individual. The algorithm presented in \cite{fiori} independently estimates the probability density functions (PDF) of the two variables by dividing the dataset ranges into bins and populating LUTs for the relative frequencies. The PDFs are then integrated to obtain the cumulative distribution functions (CDF) which are, or can be licitly adjusted to be, bijective and allow for the regression model to be obtained as the map between values with equal probabilities.

This report describes an alternative algorithm developed to make bivariate statistical regression more versatile and faster over large datasets by entirely avoiding the binning and integration operations.

The present report is organized as follows. Section \ref{blsbr} recalls the notion of bivariate statistical regression and explains the main idea and the details about the proposed binning-less bivariate statistical regression algorithm. Section \ref{gfr} explains an analysis of the glomerular-filtration-rate (GFR) estimation based on creatinine levels and illustrates such analysis by means of numerical tests performed on a dataset drawn from a study on pediatric patients in mainland China. Section \ref{conclusion} concludes the paper.
%
\section{Binning-less statistical bivariate regression algorithm}\label{blsbr}
%
Given the random variables $X$ and $Y$, for which we expect the existance of a monotonic function $f$ such that $Y=f(X)$, let $D_X \in \mathbb{R}^n$ and $D_Y \in \mathbb{R}^m$ be arrays whose components are realizations of $X$ and $Y$ respectively.

The developed regression algorithm is encapsulated in a function that takes the $D_X$ and $D_Y$ dataset arrays along with an $x$ for which the estimated value of $f(x)$ is returned.

Denoting by $P_X(x)$ and by $P_Y(y)$ the respective CDFs of $X$ and $Y$, we recall from our previous work \cite{fiori,fgl} that
\begin{equation}
  f(x)=P_Y^{-1}(P_X(x)) \text{ if $f$ is monotonically increasing},
\end{equation}
\begin{equation}
  f(x)=P_Y^{-1}(1-P_X(x)) \text{ if $f$ is monotonically decreasing}
\end{equation}
We further recall that, since the modeling solution is not unique, in general, we assumed that the center of mass of the $X$ data corresponds to the center of mass of the $Y$ data.

The regression procedure can be separated into two parts: the evaluation of a CDF and the evaluation of an inverse CDF. In this report, the former is handled by the $CDF$ function defined in Algorithm \ref{cdf_algo}, the latter by $INVCDF$ defined in Algorithm \ref{invcdf_algo}. Besides the argument for the CDF (or inverse CDF), both procedures require a dataset from which to infer the actual distributions. Algorithm \ref{regression_algo} provides pseudocode for putting the two parts together in the case that a monotonically increasing model is sought. If the model is expected to be monotonically decreasing then the value $P$ on Line 3 should be replaced by its complement $1-P$.

\begin{algorithm}
  \caption{Statistical Bivariate Regression (for monotonically increasing models)}
  \label{regression_algo}
  \begin{algorithmic}[1]
    \Function{statisticalRegression}{$D_X$,$D_Y$, $x_q$}
    \State $P \gets CDF(D_X, x_q)$
    \Comment Evaluate CDF for value $x_q$ in data set $D_X$
    \State $y_q \gets INVCDF(D_Y, P)$
    \Comment Evaluate inverse CDF for probability $P$ on data set $D_Y$
    \State \Return $y_q$
    \EndFunction
  \end{algorithmic}
\end{algorithm}
%

We developed a binningless procedure to estimate the value of the CDF $P(q)$ of a generic random variable for which $n$ realizations are stored as the components of the array $D$.
The main idea to avoid binning is to estimate the cumulative distribution function of the dataset without resorting to an estimate of the probability density function first. This can be done by embracing the definition of CDF itself, which simply leads us to counting the number of realizations that are less than or equal to $q$ and dividing by $n$. The solution shown in Algorithm \ref{cdf_algo} expands this idea to allow for a continuos, strictly monotonic interpolation for values which are not included in the original dataset. Please mind that array indexing is 1-based.

\begin{algorithm}
  \caption{Cumulative distribution function estimation}
  \label{cdf_algo}
  \begin{algorithmic}[1]
    \Function{cdf}{$D$, $q$}
    \State $D \gets sort(D)$
    \Comment Dataset is put into ascending order
    \State $n \gets length(D)$
    \State $l \gets 1$
    \State $r \gets n$
    \While {$r - l > 1$}
    \State $m \gets \left \lfloor{(l+r)/2}\right \rfloor$
    \If {$D[m] > q \land D[m]\neq D[l]$}
    \State $r \gets m$
    \Else
    \State $l \gets m$
    \EndIf
    \EndWhile


    \While{$D[r] = D[l]$}
    \State $l \gets l - 1$
    \EndWhile

    \While {$r < n \land D[r] = D[r+1]$}
    \State $r \gets r + 1$
    \EndWhile
    \State $d \gets (q-D[l])/(D[r]-D[l])$
    \State $P \gets (l + d \cdot (r - l))/n $

    % trim out of range interpolations
    \If {$P<0$}
    \State {$P\gets0$}
    \ElsIf {$P>1$}
    \State {$P\gets1$}
    \EndIf

    \State \Return $P$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Here are a few comments about Algorithm \ref{cdf_algo}:
\begin{itemize}
  \item \textbf{Line 2}: The algorithm is notably simplified by sorting the entries of $D$ into ascending order.
  \item \textbf{Lines 4--13}: This part is essentially a binary search for $q$ in array $D$. The loop starts with indexes $l$ and $r$ as the extremes of $D$ and ends with $l$ as the index of the last value less than $q$, and $r$ as that of the first value greater than $q$. The only exception that may occur is handled in lines 14--16.
  \item \textbf{Line 8}: Making sure that $D[m]\neq D[l]$ is needed to stop $l$ and $r$ converging to $1$ and $2$ respectively, in the case that $q$ is smaller than all elements in $D$.
  \item \textbf{Lines 14--16}: This loop is needed to fix $l$ in the case it converges to $n-1$ as a consequence of $q$ being greater than all values in $D$.
  \item \textbf{Lines 17--19}: The previous operations already guarantee that $l$ is the last index for the value $D[l]$, which means that there are $l$ elements in $D$ which are less than or equal to $D[l]$. This loop finds $r$, the count of elements that are less than or equal to $D[r]$.
  \item \textbf{Lines 20--21}: $P(D[l])$ can be estimated by $l/n$ whilst $P(D[r])$ can be estimated by $r/n$. $P(q)$ is obtained via linear interpolation.
  \item \textbf{Lines 22--26}: These checks limit the CDF for values outside the range of $D$.
\end{itemize}
%

\begin{algorithm}
  \caption{Inverse cumulative distribution function estimation}
  \label{invcdf_algo}
  \begin{algorithmic}[1]
    \Function{invcdf}{$D$,$P_q$}
    \State $D \gets sort(D)$
    \Comment Dataset is put into ascending order
    \State $n \gets length(D)$
    \State $p \gets P_q \cdot n$
    \State $r \gets  \left \lceil{p}\right \rceil$
    \If {$r=0$}
    \State {$r\gets1$}
    \EndIf
    \State $l \gets r$

    \While{$r < n \land D[r] = D[r+1]$}
    \State $ r \gets r + 1$
    \EndWhile

    \While {$l > 1 \land D[l - 1] = D[l]$}
    \State $l \gets l - 1$
    \EndWhile

    \If {$l = 1$}
    \State $l \gets r$
    \State $r \gets r+1$
    \While {$r < n \land D[r] = D[r+1]$}
    \State $r \gets r + 1$
    \EndWhile
    \Else
    \State $ l \gets l - 1$
    \EndIf

    \State $d \gets (p-l)/(r-l) $
    \State $q \gets D[l] + d \cdot (D[r]-D[l])$

    \State \Return $q$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Evaluation of the inverse CDF is based on the same principle only applied in reverse: the input argument is a probability, it is multiplied by $n$ and rounded to an integer $r$. If the dataset $D$ is sorted, then there will be $r$ values less than or equal to $D[r]$. Algorithm \ref{invcdf_algo} allows for a continuos, strictly monotonic interpolation to yield values which are not included in the original dataset.
Here are a few comments about Algorithm \ref{invcdf_algo}:
\begin{itemize}
  \item \textbf{Line 2}: The algorithm is notably simplified by sorting $D$ into ascending order.
  \item \textbf{Line 4}: The input probability is denormalised into the range $[0, n]$.
  \item \textbf{Lines 5--9}: $r$ and $l$ take the value of $p$ rounded to the next integer to be used as an index, this also requires $r$ and $l$ to be non-zero.
  \item \textbf{Lines 10--12}: $r$ is made to point to the last occurence of the smallest value whose CDF is greater than the requested probability.
  \item \textbf{Lines 13--15, 20}: $l$ is made to point to the last occurence of the greatest value whose CDF is less than the requested probability.
  \item \textbf{Lines 16--22}: Alignments are made to allow interpolation for small probabilities.
  \item \textbf{Lines 20--21}: $P^{-1}(l/n)$ can be estimated by $D[l]$ whilst $P^{-1}(r/n)$ can be estimated by $D[r]$. Nearby values are obtained via linear interpolation.
\end{itemize}
%

\begin{tcolorbox}[colback=gray!30,%gray background
  colframe=black,% black frame colour
  width=\dimexpr\linewidth-2\fboxrule\relax,
  arc=3mm, auto outer arc,
  breakable
  ]
  \underline{\textsc{Small numerical example}}: Let $X=[2,3,5,5,6,6,7,9]$, $Y=[6,7,10,10,11,11,11,12,15,20]$, i.e., $n=8$, $m=10$. The actual underlying model is $f(x)=2x+2$. Note that the data aren't paired. Both arrays are already sorted for simplicity. Let the query point be $q=4$, we expect $y_q\approx10$.

  Let's first estimate $P_X(4)$: the $cdf$ algorithm will find $l=2$ and $r=4$. It then computes the linear interpolation:
  %
  \begin{equation*}
    d = \tfrac{q-X[l]}{X[r]-X[l]} = \tfrac{4-3}{5-3} = 0.5
    \quad\quad
    P_X(4)=\tfrac{l+d\cdot(r-l)}{n}=\tfrac{2+0.5\cdot(4-2)}{8}=0.375
  \end{equation*}

  Now it's time to estimate $P^{-1}_Y(0.375)$: the $invcdf$ algorithm will find $p = 0.375\cdot m=3.75$, $r=4$ and $l=2$. By linear interpolation, it then finds
  %
  \begin{equation*}
    d = \tfrac{p-l}{r-l} = \tfrac{3.75-2}{4-2} = 0.875
  \end{equation*}
  \begin{equation*}
    P^{-1}_Y(0.375)=D[l]+d\cdot(D[r]-D[l])=7+0.875\cdot(10-7)=9.625
  \end{equation*}
\end{tcolorbox}

Ideally, it should hold that $INVCDF(CDF(q))=q$. To verify this identity we ran a numerical test on our Matlab implementation of Alogrithms \ref{cdf_algo} and \ref{invcdf_algo} using the $sCr$ array analysed in Section \ref{gfr}. For each element $q$ in the array, we calculated the relative deviation from identity
$$ \delta = \left|\frac{INVCDF(CDF(q)) - q}{q}\right|.$$
The greatest value of $\delta$ was found to be in the order of $10^{-15}$, which is more than acceptable. The test was repeated on the $GFR$ array analysed in Section \ref{gfr} yielding an even smaller $\delta$ of around $10^{-16}$.
%
\section{Application to glomerular filtration rate estimation}\label{gfr}

\begin{figure}[ht]
  \centering
  \makebox[\textwidth][c]{\includegraphics[scale=0.6]{figures/equations}}
  \caption{Comparison of predictions made using different equations (MDRD, CKD-EPI, Mayo Quadratic and updated Schwartz).}
  \label{equations}
\end{figure}

Existing multivariate formulas for GFR estimation have been compared and validated in \cite{gfr} over a dataset of 86 Chinese children and adolescents aged 1 through 18. The authors of the research have included their dataset with the publication, for each patient data comprises age, gender, physical parameters (such as height and weight), $GFR$ (measured using double-sample plasma clearance \cite{gold}) and two values for serum creatinine concentration (sCr). The two values of $sCr$ correspond to two different measurement tecniques: the Jaffe\cite{jaffe} and IDMS\cite{idms} methods. Units for $GFR$ and $sCr$ are mL/min and mg/dL respectively, height is in meters and age is expressed in years, this will hold throughout this report.
The study \cite{gfr} found the most effective estimation formula to be the Schwartz2009 equation \cite{schwartz}:
$$ GFR = 41.3 \cdot \frac{height}{sCr}.$$

Over said data we also computed estimations using the other three widely employed formulas that follow and compared them with the results of the Schwartz2009 estimation. The MDRD 4-variable equation \cite{MDRD}:
$$ GFR = 186 \cdot sCr^{-1.154}\cdot Age^{-0.203} \cdot [1.2010 \text{ if Black}] \cdot [0.742 \text{ if Female}].$$

The CKD-EPI formula \cite{ckdepi} ($k$ is $0.7$ for females and $0.9$ for males, $a$ is $−0.329$ for females and $−0.411$ for males):
$$ GFR = 141 \cdot min(sCr/k,1)^a\cdot max(sCr/k,1)^{-1.209}\cdot0.993^{Age}\cdot [1.018 \text{ if Female}] \cdot [1.159 \text{ if Black}].$$

The Mayo Quadratic formula \cite{mayo} (if $sCr$ is less than $0.8 mg/dL$, use $0.8 mg/dL$ for $sCr$):
$$ GFR = exp(1.911+\frac{5.249}{sCr}-\frac{2.114}{sCr^2}-0.00686\cdot Age - [0.205 \text{ if Female}]).$$

Looking at Figure \ref{equations}, it is clear that the Schwartz2009 equation outperforms all of the other functions.

In order to apply SBR, we must first assess the existance of a single dominant variable. This was clearly found to be the serum creatinine concentration. Other variables used to estimate GFR are age and height, whose effect however is marginal, as the scatter plots in Figure \ref{recessive} reveal no strong statistical features. Quantitatively this is confirmed by the population correlation coefficient \cite{mukaka}, which, for a generic paired dataset $(z_i, y_i)$ for $i = 1, 2, \ldots, n$ is calculated as:

$$ \rho_{y,z} = \frac{\sum_{i = 1}^n{(z_i-\bar{z})(y_i-\bar{y})}}
{\sqrt{\sum_{i=1}^n{(z_i-\bar{z})^2}\sum_{i=1}^n{(y_i-\bar{y})^2}}}.$$

$\rho$ belongs to the interval $[-1,1]$. If variables are directly correlated then we expect the coefficient to approach $+1$. If they are inversely correlated we expect something close to $-1$. Unrelated variables should yield $\rho$ close to $0$. Table \ref{corr} shows correlation coefficients between GFR and each of age, height and sCr for the whole population and the gender-defined subsets.

\begin{table}[ht]
  \centering
  \begin{tabular}{|l|c|c|c|}
    \hline
    $Gender$&$\rho_{GFR,sCr}$&$\rho_{GFR,age}$&$\rho_{GFR,height}$\\
    \hline
    Males     & -0.7051 & -0.1375 & -0.0910  \\
    \hline
    Females   & -0.7249 &  0.0801 &  0.0548  \\
    \hline
    Both      & -0.6744 & -0.0565 & -0.0425  \\
    \hline
  \end{tabular}
  \caption{Population correlation coefficients between GFR and each of age, height and sCr.}
  \label{corr}
\end{table}

\begin{figure}[ht]
  \centering
  \makebox[\textwidth][c]{\includegraphics[scale=0.6]{figures/recessive}}
  \caption{Plotting GFR as a function of Age or Height does not show very strong correlation.}
  \label{recessive}
\end{figure}

As SBR generates a univariate model, for the sake of the comparison, the Schwartz2009 equation was simplified to be independent of height. This was done by replacing the variable with a constant equal to the mean height of all individuals in the dataset. This model was plotted in Figure \ref{regression}, along with the datapoints, the regression curve obtained by SBR using the Numerical-Algebraic Neural System (NANS) method explained in \cite{fiori} and that obtained using the binning-less method described in Section 2.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.6]{figures/regression}
  \caption{Data set with overlaid regression and estimation curves (comparison with the updated Schwartz equation, the simplified Schwartz equation, the method of \cite{fiori} and the proposed method)}
  \label{regression}
\end{figure}

The four models (plotted in Figure \ref{regression}) are compared on prediction performance using three indexes: mean squared error ($MSE$), mean absolute error ($MAE$) and coefficient of determination ($R^2$). For a generic model $f(x)$, fitting the set of datapoints $\lbrace(x_1,y_1), (x_2,y_2), \ldots, (x_n,y_n)\rbrace$, these indexes can be calculated as follows.
$$ MSE = \frac{1}{n}\sum_{i=1}^{n}{(f(x_i)-y_i)^2}.$$
$$ MAE = \frac{1}{n}\sum_{i=1}^{n}{|f(x_i)-y_i|}.$$
$$ R^2 = 1 - \frac{\sum_{i=1}^{n}{(f(x_i)-y_i)^2}}
                  {\sum_{i=1}^{n}{(f(x_i)-\bar{y})^2}}
,\hspace{20pt}\text{where } \bar{y} = \frac{1}{n}\sum_{i=1}^{n}{y_i} $$

The Schwartz2009 curve has the lowest $R^2$ as it also takes height into account, the SBR regressions both have a better $MSE$ as they were obtained from the same data we are now validating them on, that shows that SBR is very effective at fitting data.

Comparisons were also made to evaluate the generality of the models obtained, this was done by measuring the "roughness" of the curve, using the index
$$ G = \sqrt{\sum_{i=3}^{N}{\frac{(f_i-2f_{i-1}+f_{i-2})^2}{N-2}}},$$
which is defined as the root mean square of the second order differences of the sequence $f_i$, so $G$ increases with sharp changes in slope. To be useful, the $f_i$ values have to be sorted in some significant manner, in our case, for each model $f(x)$ to be evaluated, $f_i$ assumes the values interpolated at $N$ equally spaced, increasing, values of serum creatinine concentration:
$$ f_i = f(x_{min} + (i-1)\frac{x_{max}-x_{min}}{N-1}) \text{ for } i = 1 \ldots N.$$
Where $x_{min}$ and $x_{max}$ are respectively the smallest and greatest measured creatinine concentrations:
$$x_{min} = \min_{i = 1 \ldots n}{x_i},\hspace{20pt}x_{max} = \max_{i = 1 \ldots n}{x_i}.$$
An index similar to $G$ is also used in \cite{bishop} to prevent overfitting.
The value of $G$ is expected to be greater for irregular curves and indeed it is close to zero for the simplified Schwartz2009 model (independent of height), which is essentially a hyperbola, graph of a smooth function, while it is very high for the Schwartz2009 curve, as we are including the effect of a second independent variable (which does contribute to ordering $f_n$).

\begin{table}
  \centering
  \begin{tabular}{|l|c|c|c|c|}
    \hline
    $Model$                    &$G$    &$MSE$  &$MAE$  &$R^2$    \\
    \hline
    Schwartz2009               &14.69  &863.92 &23.19  & 0.1491  \\
    Schwartz2009 (mean height) &0.3622 &1341.8 &27.84  &-0.3216  \\
    Binning-less SBR           &1.652  &674.90 &20.19  & 0.3353  \\
    NANS SBR                   &1.836  &696.62 &20.32  & 0.3139  \\
    \hline
  \end{tabular}
  \caption{Roughness ($G$), mean squared error ($MSE$), mean absolute error ($MAE$) and coefficient of determination ($R^2$) for the four estimation models.}
  \label{regstats}
\end{table}
%
\section{Conclusions}\label{conclusion}
%
The aim of the present paper is to discuss the bivariate statistical regression method and to provide an improved algorithm which does not rely on binning for the steps which require computation of the estimated distribution functions. The new algorithm was then compared to the original bivariate statistical regression based on numerical-algebraic neural systems in the application to a real-world data set. The comparison proved an all-round improvement in the new method, which, aside from being more efficient, yields a closer fit and a smoother curve.
%
\begin{thebibliography}{99}
  \bibitem{bestchak} M.J. Best, N. Chakravarti, ``Active set algorithms for isotonic regression; a unifying framework``, \textit{Mathematical Programming}, 47: 425–439, 1990
  \bibitem{bishop} C.M. Bishop, ``Training with noise is equivalent to Tikhonov regularization'', \textit{Neural Computation}, Vol. 7, No. 1, pp. 108 -- 116, 1995
  \bibitem{mgfr2}Z.H. Endre, J.W. Pickering, R.J. Walker, ``Clearance and beyond: the complementary roles of GFR measurement and injury biomarkers in acute kidney injury (AKI)``, \textit{American Journal of Physiology}, Vol. 301 no. 4, 2011
  \bibitem{fiori} S. Fiori, ``Fast statistical regression in presence of a dominant independent variable'', \textit{Neural Computing and Applications}, Vol. 22, No. 7, pp. 1367 -- 1378, 2013
  \bibitem{fgl} S. Fiori, T. Gong and H.K. Lee, ``Bivariate nonisotonic statistical regression by a lookup table neural system'', \textit{Cognitive Computation}, Vol. 7, No. 6, pp. 715 -- 730, 2015
  \bibitem{seismic} D. Gao, ``Texture model regression for effective feature discrimination: Application to seismic facies visualization and interpretation``, \textit{GEOPHYSICS}, 69(4), pp. 958-967, 2004
  \bibitem{gfrmed}J. Gill, R. Malyuk, O. Djurdjev, A. Levin, ``Use of GFR equations to adjust drug doses in an elderly multi-ethnic group—a cautionary tale``, \textit{Nephrology Dialysis Transplantation}, Vol. 22, Issue 10, pp. 2894–2899, 2007
  \bibitem{book}F.E. Harrell, ``Regression modeling strategies: with applications to linear models, logistic and ordinal regression, and survival analysis.`` \textit{Springer}, 2015

  \bibitem{fat} I. Janssen, P. T Katzmarzyk, R. Ross, ``Waist circumference and not body mass index explains obesity-related health risk``\textit{American Society for Clinical Nutrition}, Vol. 79, pp. 379-384, 2004

  \bibitem{control}R.E. Kopp, R.J. Orford, ``Linear regression applied to system identification for adaptive control systems``, \textit{AIAA Journal}, Vol. 1, No. 10, pp. 2300-2306, 1963
  \bibitem{MDRD} A.S. Levey, J.P. Bosch, J.B. Lewis, T. Greene, N. Rogers, D. Roth, et al., ``A More Accurate Method To Estimate Glomerular Filtration Rate from Serum Creatinine: A New Prediction Equation.`` \textit{Ann Intern Med}, 130:461–470, 1999
  \bibitem{ckdepi} A.S Levey, L.A. Stevens, C.H. Schmid, et al., ``A New Equation to Estimate Glomerular Filtration Rate``, \textit{Annals of internal medicine}, 150(9):604-612, 2009
  \bibitem{pava} P. Mair, K. Hornik and J. de Leeuw, ``Isotone Optimization in R: Pool-Adjacent-Violators Algorithm (PAVA) and Active Set Methods``, \textit{Journal of Statistical Software}, 32 (5). pp. 1-24, 2009
  \bibitem{ts} E. Masry, ``Multivariate local polynomial regression for time series: uniform strong consistency and rates.``, \textit{Journal of Time Series Analysis}, 17: 571–599, 1996
  \bibitem{gfralb} K. Matsushita, M. van der Velde, B.C. Astor, M. Woodward, A.S. Levey, P.E. de Jong, J. Coresh, R.T. Gansevoort, ``Association of estimated glomerular filtration rate and albuminuria with all-cause and cardiovascular mortality in general population cohorts: a collaborative meta-analysis``, \textit{The Lancet}, Volume 375, Issue 9731, pp. 2073-2081, 2010
  \bibitem{strontium}J.M. McArthur, R.J. Howarth, T.R. Bailey, ``Strontium Isotope Stratigraphy: LOWESS Version 3: Best Fit to the Marine Sr‐Isotope Curve for 0–509 Ma and Accompanying Look‐up Table for Deriving Numerical Age``, \textit{The Journal of Geology} 109:2, pp. 155-170, 2001
  \bibitem{mukaka} M.M. Mukaka, ``A guide to appropriate use of correlation coefficient in medical research.``, \textit{Malawi Medical Journal}, 24(3), 69-71, 2012
  \bibitem{mayo}A.D Rule, T.S. Larson, E.J. Bergstralh, J.M. Slezak, S.J. Jacobsen, F.G. Cosio, ``Using serum creatinine to estimate glomerular filtration rate: accuracy in good health and in chronic kidney disease``, \textit{Annals of Internal Medicine}, 141 (12): 929–37, 2004
  \bibitem{schwartz} G.J. Schwartz, A. Muñoz, M.F. Schneider, R.H. Mak, F. Kaskel, B.A. Warady, S.L. Furth, "New equations to estimate GFR in Children with CKD", \textit{Journal of the American Society of Nephrology}, Vol. 20, No. 3, pp. 629 – 637, 2009
  \bibitem{gold}M.A. Serdar, I. Kurt, F. Ozcelik, M. Urhan, S. Ilgan, M. Yenicesu, T. Kutluay, ``A practical approach to glomerular filtration rate measurements: creatinine clearance estimation using cimetidine``, \textit{Annals of Clinical \& Laboratory Science}, 31(3), 265-273, 2001
  \bibitem{jaffe} C. Slot, ``Plasma creatinine determination a new and specific Jaffe reaction method.`` \textit{Scandinavian journal of clinical and laboratory investigation}, 17.4: 381-387, 1965
  \bibitem{mgfr}I. Soveri, U.B. Berg, J. Björk, C.G. Elinder, A. Grubb, I. Mejare, G. Sterner, S.E. Bäck, ``Measuring GFR: A Systematic Review``, \textit{American Journal of Kidney Diseases}, Vol. 64, Issue 3, pp. 411-424, 2014
  \bibitem{idms} M.J. Welch et al., ``Determination of serum creatinine by isotope dilution mass spectrometry as a candidate definitive method``, \textit{Analytical chemistry}, 58.8: 1681-1685, 1986
  \bibitem{gfr} K. Zheng, M. Gong, Y. Qin, H. Song, X. Shi, Y. Wu, F. Li and X. Li, ``Validation of glomerular filtration rate-estimating equations in Chinese children'', \textit{PLoS ONE}, Vol. 12, No. 7, pp. e0180565 (\doi{10.1371/journal.pone.0180565}), 2017
  %
\end{thebibliography}
\end{document}
