% Compilare con PDFLaTeX
\documentclass[10pt,final]{siamltex}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{amsmath,amssymb}
%\usepackage{gensymb}
\textwidth = 16cm
\usepackage{pstricks}
%\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[breakable]{tcolorbox}
\usepackage{doi}
%
\begin{document}
\title{Glomerular filtration rate estimation by a novel binning-less bivariate isotonic statistical regression method}
\author{Sebastian~Giles, Simone~Fiori%
\thanks{S. Giles is with the School of Information and Automation Engineering,
Universit\`{a} Politecnica delle Marche,
Via Brecce Bianche, I-60131 Ancona (Italy).
\newline\indent
S. Fiori is with Dipartimento di Ingegneria dell'Informazione,
Universit\`{a} Politecnica delle Marche,
Via Brecce Bianche, I-60131 Ancona (Italy).
\newline\indent
This draft is dated \today.}}
\maketitle
\def\bbbr{\mathbb{R}}
\def\bbbx{\mathbb{X}}
\def\bbby{\mathbb{Y}}
\def\mdef{{\stackrel{{\mathrm{def}}}{=}}}
% Footnotes in non-numerical fashion
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
\def\to{\mathbf{\ to\ }}
\setcounter{footnote}{1}
%
%
\begin{abstract}
  {\red L'abstract contiene un riassunto molto stringato del problema affrontato, dell'applicazione e dei risultati.}
\end{abstract}
%
\section{Introduction}
Several real-world phenomena lack accurate mathematical descriptions. In these cases it is not possible to predict the value of a variable by plugging known quantities in to an analytically derived expression, therefore statistical methods must be employed to develop a model of the observed process. The most common set of tools used to infer a functional relationship between variables is regression analysis, here only bivariate regression techniques will be considered. The amount of available data is assumed to be enough to explain the relevant statistical features of the phenomenon underlying the data.

Traditional forms of regression are based on some parametrised function whose graph is made to lie reasonably close to the points making up the experimental dataset. Values for the parameters are typically found using the least squares method.

Isotonic regression allows greater freedom for the regression curve to fit data by constructing a piecewise linear function, described by a lookup table (LUT). Overfitting is avoided by requiring the function to be monotonic, this is obviously also a limit on the process we want to model. Various algorithms can be used to find the LUT values that satisfy a least squares condition.

Statistical bivariate regression (SBR) constitutes an improvement over isotonic regression, its advantages derive from the fact that it relies on finding the relationship between the statistical distributions of the variables. SBR is not based on a least squares method so it does not require data to be associated in ordered pairs, this makes it ideal for correlating two quantities that cannot be both measured on the same individual. The algorithm presented in \cite{fiori} independently estimates the probability density functions (PDF) of the two variables by dividing the dataset ranges into bins and populating LUTs for the relative frequencies, the PDFs are then integrated to obtain the cumulative distribution functions (CDF) which are, or can be licitly adjusted to be, bijective and allow for the regression model to be obtained as the map between values with equal probabilities.

This report describes an alternative algorithm developed to make bivariate statistical regression more versatile and faster over large datasets by entirely avoiding the binning and integration operations.

Glomerular filtration rate (GFR) is used as an indicator of kidney function, as such it is relevant for  assessing progression of renal disease, it is also frequently required for evaluating optimal dosage for medications. However, determination of true GFR is time-consuming, costly, and difficult to perform. Thus, there is considerable interest in developing formulas to estimate GFR using simpler parameters such as age, weight, height and sex and values which can be more conveniently measured as part of a blood test.

The present report is organized as follows. Section 2 recalls the notion of bivariate statistical re- gression and explains the main idea and the details about the proposed binning-less bivariate statistical regression algorithm. Section 3 explains an analysis of the glomerular-filtration-rate (GFR) estimation based on creatinine levels and illustrates such analysis by means of numerical tests performed on a dataset drawn from a study on pediatric patients in mainland China. Section 4 concludes the paper.
%
\section{Binning-less bivariate statistical regression algorithm}
%
Given the random variables $X$ and $Y$, for which we expect the existance of a monotonic function $f$ such that $Y=f(X)$, let $D_X \in \mathbb{R}^n$ and $D_Y \in \mathbb{R}^m$ be vectors whose components are realizations of $X$ and $Y$ respectively.

The developed regression algorithm is encapsulated in a function that takes the $D_X$ and $D_Y$ dataset vectors along with an $x$ for which the estimated value of $f(x)$ is returned.

Denoting by $P_X(x)$ and by $P_Y(y)$ the respective CDFs of $X$ and $Y$, we recall from our previous work that
\begin{equation}
  f(x)=P_Y^{-1}(P_X(x)) \text{ if $f$ is monotonically increasing},
\end{equation}
\begin{equation}
  f(x)=P_Y^{-1}(1-P_X(x)) \text{ if $f$ is monotonically decreasing}
\end{equation}
We further recall that, since the modeling solution is not unique, in general, we assumed that the center of mass of the $X$ data corresponds to the center of mass of the $Y$ data.

The regression procedure can be separated into two parts: the evaluation of a CDF and the evaluation of an inverse CDF. In this report the former is handled by the $cdf$ function defined in Algorithm \ref{cdf_algo}, the latter by $invcdf$ defined in Algorithm \ref{invcdf_algo}. Besides the argument for the CDF (or inverse CDF) both procedures require a dataset from which to infer the actual distributions. Algorithm \ref{regression_algo} provides pseudocode for putting the two parts together.

\begin{algorithm}
  \caption{Statistical Bivariate Regression}
  \label{regression_algo}
  \begin{algorithmic}[1]
    \Function{statisticalRegression}{$D_x$,$D_y$, $x_q$}
    \State $P \gets cdf(D_x, Q_x)$
    \Comment Evaluate CDF for value $Q_x$ in data set $D_x$
    \State $y_q \gets invcdf(D_y, P)$
    \Comment Evaluate inverse CDF for probability $P$ on data set $D_y$
    \State \Return $y_q$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\section{Cumulative distribution function estimation algorithm}
%
This section explains a procedure to estimate the value of the CDF $P(q)$ of a generic random variable for which $n$ realizations are stored as the components of the vector $D$.
The main idea to avoid binning is to estimate the cumulative distribution function of the dataset without resorting to an estimate of the probability density function first, this can be done by embracing the definition of CDF itself, which simply leads us to counting the number of realizations that are less than or equal to $q$ and dividing by $n$. The solution shown in algorithm \ref{cdf_algo} expands this idea to allow for a continuos, strictly monotonic interpolation for values which are not included in the original dataset. Please mind that 1-based vectors are used.

\begin{algorithm}
  \caption{Cumulative distribution function estimation}
  \label{cdf_algo}
  \begin{algorithmic}[1]
    \Function{cdf}{$D$, $q$}
    \State $D \gets sort(D)$
    \Comment Dataset is put into ascending order
    \State $n \gets length(D)$
    \State $l \gets 1$
    \State $r \gets n$
    \While {$r - l > 1$}
    \State $m \gets \left \lfloor{(l+r)/2}\right \rfloor$
    \If {$D[m] > q \land D[m]\neq D[l]$}
    \State $r \gets m$
    \Else
    \State $l \gets m$
    \EndIf
    \EndWhile


    \While{$D[r] = D[l]$}
    \State $l \gets l - 1$
    \EndWhile

    \While {$r < n \land D[r] = D[r+1]$}
    \State $r \gets r + 1$
    \EndWhile
    \State $d \gets (q-D[l])/(D[r]-D[l])$
    \State $p \gets (l + d \cdot (r - l))/n $

    % trim out of range interpolations
    \If {$p<0$}
    \State {$p\gets0$}
    \ElsIf {$p>1$}
    \State {$p\gets1$}
    \EndIf

    \State \Return $p$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Here are a few comments about Algorithm \ref{cdf_algo}:
\begin{itemize}
  \item \textbf{Line 2}: The algorithm is notably simplified by sorting $D$ into ascending order.
  \item \textbf{Lines 4--13}: This part is essentially a binary search for $q$ in vector $D$. The loop starts with indexes $l$ and $r$ as the extremes of $D$ and ends with $l$ as the index of the last value less than $q$, and $r$ as that of the first value greater than $q$. The only exception that may occur is handled in lines 14--16.
  \item \textbf{Line 8}: Making sure that $D[m]\neq D[l]$ is needed to stop $l$ and $r$ converging to $1$ and $2$ respectively, in the case that $q$ is smaller than all elements in $D$.
  \item \textbf{Lines 14--16}: This loop is needed to fix $l$ in the case it converges to $n-1$ as a consequence of $q$ being greater than all values in $D$.
  \item \textbf{Lines 17--19}: The previous operations already guarantee that $l$ is the last index for the value $D[l]$, this means there are $l$ elements in D which are less than or equal to $D[l]$. This loop finds $r$, the count of elements that are less than or equal to $D[r]$.
  \item \textbf{Line 20-21}: $P(D[l])$ can be estimated by $l/n$ whilst $P(D[r])$ can be estimated by $r/n$, $P(q)$ is obtained via linear interpolation.
  \item \textbf{Lines 22--26}: These checks limit the CDF for values outside the range of $D$.
\end{itemize}
%

\begin{algorithm}
  \caption{Inverse Cumulative distribution function estimation}
  \label{invcdf_algo}
  \begin{algorithmic}[1]
    \Function{invcdf}{$D$,$P_q$}
    \State $D \gets sort(D)$
    \Comment Dataset is put into ascending order
    \State $n \gets length(D)$
    \State $p \gets P_q \cdot n$
    \State $r \gets  \left \lceil{p}\right \rceil$
    \If {$r=0$}
    \State {$r\gets1$}
    \EndIf
    \State $l \gets r$

    \While{$r < n \land D[r] = D[r+1]$}
    \State $ r \gets r + 1$
    \EndWhile

    \While {$l > 1 \land D[l - 1] = D[l]$}
    \State $l \gets l - 1$
    \EndWhile

    \If {$l = 1$}
    \State $l \gets r$
    \State $r \gets r+1$
    \While {$r < n \land D[r] = D[r+1]$}
    \State $r \gets r + 1$
    \EndWhile
    \Else
    \State $ l \gets l - 1$
    \EndIf

    \State $d \gets (p-l)/(r-l) $
    \State $q \gets D[l] + d \cdot (D[r]-D[l])$

    \State \Return $q$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Evaluation of the inverse CDF is based on tha same principle only applied in reverse: the input argument is a probability, it is multiplied by $n$ and rounded to an integer $r$. If the dataset $D$ is sorted, then there will be $r$ values less than or equal to $D[r]$. Algorithm \ref{invcdf_algo} allows for a continuos, strictly monotonic interpolation to yield values which are not included in the original dataset.
Here are a few comments about Algorithm \ref{invcdf_algo}:
\begin{itemize}
  \item \textbf{Line 2}: The algorithm is notably simplified by sorting $D$ into ascending order.
  \item \textbf{Line 4}: The input probability is denormalised into the range $[0, n]$.
  \item \textbf{Lines 5--9}: $r$ and $l$ take the value of $p$ rounded to the next integer to be used as an index, this also requires $r$ and $l$ to be non-zero.
  \item \textbf{Lines 10--12}: $r$ is made to point to the last occurence of the smallest value whose CDF is greater than the requested probability.
  \item \textbf{Lines 13--15, 20}: $l$ is made to point to the last occurence of the greatest value whose CDF is less than the requested probability.
  \item \textbf{Lines 16--22}: Allignments are made to allow interpolation for small proabilities.
  \item \textbf{Line 20-21}: $P^-1(l/n)$ can be estimated by $D[l]$ whilst $P^-1(r/n)$ can be estimated by $D[r]$, nearby values are obtained via linear interpolation.
\end{itemize}
%

\begin{tcolorbox}[colback=gray!30,%gray background
  colframe=black,% black frame colour
  width=\dimexpr\linewidth-2\fboxrule\relax,
  arc=3mm, auto outer arc,
  breakable
  ]
  \underline{\textsc{Small numerical example}}: Let $X=[2,3,5,5,6,6,7,9]$, $Y=[6,7,10,10,11,11,11,12,15,20]$, i.e., $n=8$, $m=10$. The actual underlying model is $f(x)=2x+2$. Note that the data aren't paired. Both vectors are already sorted for simplicity. Let the query point be $q=4$, we expect $y_q\approx10$.

Let's first estimate $P_X(4)$: the $cdf$ algorithm will find $l=2$ and $r=4$. It then computes the linear interpolation:
  %
  \begin{equation*}
    d = \tfrac{q-X[l]}{X[r]-X[l]} = \tfrac{4-3}{5-3} = 0.5
    \quad\quad
    P_X(4)=\tfrac{l+d\cdot(r-l)}{n}=\tfrac{2+0.5\cdot(4-2)}{8}=0.375
  \end{equation*}

Now it's time to estimate $P^-1_Y(0.375)$: the $invcdf$ algorithm will find $p = 0.375  m=3.75$, $r=4$ and $l=2$. By linear interpolation, it then finds
    %
    \begin{equation*}
      d = \tfrac{p-l}{r-l} = \tfrac{3.75-2}{4-2} = 0.875
    \end{equation*}
    \begin{equation*}
      P^-1_Y(0.375)=D[l]+d\cdot(D[r]-D[l])=7+0.875\cdot(10-7)=9.625
    \end{equation*}
\end{tcolorbox}
%
\section{Application to glomerular filtration rate estimation}\label{sec3}
%
{{\red Questa sezione presenta i dati, le loro rappresentazioni grafiche, i risultati delle regressioni, i confronti con i modelli analitici, e i relativi commenti esplicativi. Qui vanno spiegati anche gli indici di prestazione MSE e Roughness. Per quest'ultimo, si puo' fare riferimento alla regolarizzazione di Tikhonov riassunta in \cite{bishop}. Le figure, fatte con MATLAB, possono essere salvate in formato PDF e caricate all'interno del documento come mostrato sotto. Idem per la tabella che può essere salvata dal MATLAB direttamente in formato LaTeX.}}

An example of result of application of the Algorithm~\ref{staircase_algo_mi} to a real-world dataset is shown in the Figure~\ref{bodyfat}.

\vspace{5mm}
%
\begin{figure}[h!]
  \centering
  \includegraphics[scale=0.3]{Experiment1a}
  \caption{Example of figure.}
  \label{bodyfat}
\end{figure}
%
%
\section{Conclusions}\label{sec4}
%
{{\red Questa sezione riassume i risultati, sia concettuali che pratici, ottenuti.}}
%
\begin{thebibliography}{99}
  %
  \bibitem{fiori} S. Fiori, ``Fast statistical regression in presence of a dominant independent variable'', \textit{Neural Computing and Applications}, Vol. 22, No. 7, pp. 1367 -- 1378, 2013
  \bibitem{bishop} C.M. Bishop, ``Training with noise is equivalent to Tikhonov regularization'', \textit{Neural Computation}, Vol. 7, No. 1, pp. 108 -- 116, 1995
  \bibitem{fgl} S. Fiori, T. Gong and H.K. Lee, ``Bivariate nonisotonic statistical regression by a lookup table neural system'', \textit{Cognitive Computation}, Vol. 7, No. 6, pp. 715 -- 730, 2015
  \bibitem{gfr_plos_one} K. Zheng, M. Gong, Y. Qin, H. Song, X. Shi, Y. Wu, F. Li and X. Li, ``Validation of glomerular filtration rate-estimating equations in Chinese children'', \textit{PLoS ONE}, Vol. 12, No. 7, pp. e0180565 (\doi{10.1371/journal.pone.0180565}), 2017
  %
\end{thebibliography}
\end{document}
